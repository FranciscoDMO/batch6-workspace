{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7527be455d372803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "In times where even [facebook goes down](https://www.theguardian.com/technology/2021/oct/04/facebook-instagram-whatsapp-outage-what-to-do), books are our reliable companions. For true book lovers, it's hard to resist the urge to buy huge piles to add to our \"To read\" lists. But how to know which is the right book to buy?\n",
    "\n",
    "In any online book store, you can find hundreds of reviews. You also have [goodreads](https://www.goodreads.com/), [book riot](https://bookriot.com/), and many others.\n",
    "\n",
    "\n",
    "\n",
    "The only problem is that it is hard to navigate through the hundreds and hundreds of reviews, and even if you did have the time, the huge amount of reviews is completely unhelpful. Not to mention the ones that definitely seem posted by bots.\n",
    "\n",
    "<img src=\"media/fake-reviews.jpg\" width=\"50%\" />\n",
    "\n",
    "\n",
    "## Q1 - Baseline \n",
    "\n",
    "To cope with all of this, you set out to create a model to look for the most helpful reviews for you. You find a dataset online providing reviews and helpfulness metrics and start there.\n",
    "\n",
    "Load the dataset and check its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6273f8aa182f664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Old Bear; Kevin Henkes (2008) Harper Collins C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This book is based on the sad truth of the sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>When Lara's grandmother's ghost begins bashing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Charlotte Simmons is the definitive, classic A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Much too stereotypical for my tastes and the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpfulness                                         reviewText\n",
       "0            1  Old Bear; Kevin Henkes (2008) Harper Collins C...\n",
       "1            0  This book is based on the sad truth of the sta...\n",
       "2            1  When Lara's grandmother's ghost begins bashing...\n",
       "3            1  Charlotte Simmons is the definitive, classic A...\n",
       "4            0  Much too stereotypical for my tastes and the s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads dataframe of reviews helpfulness \n",
    "    \"\"\"\n",
    "    df = pd.read_csv('datasets/book_review_helpfulness.csv')\n",
    "    df.head()\n",
    "    return df\n",
    "\n",
    "df = load_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0f698ca70b9e015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.a)\n",
    "\n",
    "First thing you decide to do is to extract features from the reviews. So you start working out how to do it. You think that it may be good to preprocess the text a bit, so you look into a few options. You end up converging to the following:\n",
    "\n",
    "- tokenizing the text, using WordPunctTokenizer\n",
    "- lowercasing the text\n",
    "\n",
    "You then decide to feed this into a CountVectorizer. Implement this function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1492b0bfab58773c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(X_train, X_test):\n",
    "    \"\"\"Converts the provided text training and test data into \n",
    "    feature counts. Additionally, returns the used vectorizer, \n",
    "    the processed dataframes and the number of extracted features.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: dataframe: training data, containing a \"reviewText\" column\n",
    "        X_test: dataframe: test data, containing a \"reviewText\" column\n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: fitted count vectorizer \n",
    "        num_features: number of features used by the vectorizer, for a sanity check\n",
    "        X_train_vec: vectorized training features\n",
    "        X_test_vec: vectorized test features\n",
    "    \"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    # fit the vectorizer on the training data and transform it\n",
    "    X_train_vec = vectorizer.fit_transform(X_train[\"reviewText\"])\n",
    "    \n",
    "    # transform the test data using the fitted vectorizer\n",
    "    X_test_vec = vectorizer.transform(X_test[\"reviewText\"])\n",
    "    \n",
    "    # get the number of features used by the vectorizer\n",
    "    num_features = len(vectorizer.get_feature_names())\n",
    "    \n",
    "    return vectorizer, num_features, X_train_vec, X_test_vec\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-761d12c2e662383e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['helpfulness']), \n",
    "    df['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[12, :]), 284), 'The data is not vectorized correctly.'\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[300, :]), 157), 'The data is not vectorized correctly.'\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[1411, :]), 222), 'The data is not vectorized correctly.'\n",
    "\n",
    "assert num_features == 44103, 'The number of features is not correct.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2c6d3c1f721b3d73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the next exercises, we'll provide a different version of the preprocessed data.\n",
    "\n",
    "Load the preprocessed dataset from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efe125f9b6c0960e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>old bear kevin henkes harper collins childrenl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>book based sad truth state country humor berni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>lara grandmother ghost begins bashing guests f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>charlotte simmons definitive classic american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>much stereotypical tastes storyline unbelievab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   helpfulness                                         reviewText\n",
       "0            1  old bear kevin henkes harper collins childrenl...\n",
       "1            0  book based sad truth state country humor berni...\n",
       "2            1  lara grandmother ghost begins bashing guests f...\n",
       "3            1  charlotte simmons definitive classic american ...\n",
       "4            0  much stereotypical tastes storyline unbelievab..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\"\n",
    "    Loads dataframe of preprocessed helpfulness review \n",
    "    \"\"\"\n",
    "    preprocessed_df = pd.read_csv('datasets/book_review_helpfulness_preprocessed.csv')\n",
    "    preprocessed_df.head()\n",
    "    return preprocessed_df\n",
    "\n",
    "df_preprocessed = load_preprocessed_data()\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28fc4a869d434823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.b)\n",
    "\n",
    "You now want to use your newly found features to build a baseline. Create a function that receives your vectorized dataset and trains a naïve Bayes model.\n",
    "\n",
    "First we'll start by defining a function to obtain precision and recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-93c0f5ba86c7ff62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall(y_test, y_pred):\n",
    "    \"\"\"Returns the precision and recall of the helpfulness class (label = 1)\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        y_pred (Series): Predictions corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a99d737525b250d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Implement below the function to train a naive Bayes model on the vectorized dataset and use the get_precision_recall function to return these metrics over the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4212f8a48fad26c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Returns a fitted Multinomial Naive Bayes model, the predictions on the test set\n",
    "    and the precision and recall scores for these predictions\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (Series): Vectorized text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test_vec (Series): Vectorized text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        clf (MultinomialNB): MultinomialNB classifier fitted to the vectorized training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "        precision (float): The precision score of the helpfulness class (1) on the test data\n",
    "        recall (float): The recall score of the helpfulness class (1) on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    \n",
    "    # fit the model on the training data\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # make predictions on the test data\n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    \n",
    "    # get the precision and recall scores\n",
    "    \n",
    "    precision,recall = get_precision_recall(y_test, y_pred)\n",
    "    \n",
    "    return clf, y_pred, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb35e27414e100a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Test your model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fac3a6ee5b15c471",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['helpfulness'], test_size=0.2, random_state=42)\n",
    "\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "assert math.isclose(precision, 0.6305625524769102, rel_tol=1e-05), 'Predicted precision is not correct.'\n",
    "assert math.isclose(recall, 0.689623507805326, rel_tol=1e-05), 'Predicted recall is not correct.'\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['helpfulness'], test_size=0.5, random_state=42)\n",
    "\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "assert math.isclose(precision, 0.6058282208588958, rel_tol=1e-05), 'Predicted precision is not correct.'\n",
    "assert math.isclose(recall, 0.7391467065868264, rel_tol=1e-05), 'Predicted recall is not correct.'\n",
    "\n",
    "df_prep = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_prep['reviewText'], df_prep['helpfulness'], test_size=0.2, random_state=42)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "assert math.isclose(precision, 0.6260032102728732, rel_tol=1e-05), 'Predicted precision is not correct.'\n",
    "assert math.isclose(recall, 0.7162534435261708, rel_tol=1e-05), 'Predicted recall is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1124edb4314d1b0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now run the two functions you built with your preprocessed data and check your baseline scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c1ed06f12f0f2d2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 44103\n",
      "Baseline precision: 0.6274292059966685\n",
      "Baseline recall: 0.7040498442367601\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(columns=['helpfulness']), \n",
    "    df['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "print(f\"Number of features: {num_features}\")\n",
    "print(f\"Baseline precision: {precision}\")\n",
    "print(f\"Baseline recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-652e101fb2ad549e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll also run with our own preprocessed data so you have a base for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f051bf3811e71cdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 43290\n",
      "Baseline precision: 0.6101159114857745\n",
      "Baseline recall: 0.7214953271028037\n"
     ]
    }
   ],
   "source": [
    "df_prep = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_prep['reviewText'], \n",
    "    df_prep['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train)\n",
    "X_train_vec = cv.transform(X_train)\n",
    "X_test_vec = cv.transform(X_test)\n",
    "\n",
    "print(f\"Number of features: {len(cv.vocabulary_)}\")\n",
    "\n",
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "\n",
    "print(f\"Baseline precision: {precision}\")\n",
    "print(f\"Baseline recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a27675ff9718a6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we'll use the preprocessed data for the following exercises consider this the baseline:\n",
    "\n",
    "* Baseline precision: 0.6101159114857745\n",
    "* Baseline recall: 0.7214953271028037\n",
    "\n",
    "\n",
    "## Q2) Feature analysis and selection\n",
    "\n",
    "Those are not bad results to start with, but as you've learned about feature selection you want to try it out. After all, 43 thousand features is still a pretty large number. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c58f1a74e4aeb4c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You start with a model based feature selection. For that purpose, you want to use the capability of TF-IDF/CountVectorizer to select the features to use. \n",
    "\n",
    "### Q2.a)\n",
    "\n",
    "Fit a TF-IDF vectorizer with a specific number of features. Note that the features will be selected based on the term frequency, so the resulting vocabulary will be based on the term count. Return these features sorted by inverse document frequency, the other measure used by TF-IDF - as you've seen in the learning notebooks, a measure of whether a term is common or rare in the documents.\n",
    "\n",
    "Create a function below to:\n",
    "* train a TF-IDF vectorizer on the provided train data (passing a parameter to define the maximum number of features to use)\n",
    "* return its features sorted by their inverse document frequency in descending order. \n",
    "\n",
    "**Note**: In case of a tie, that is, if several features have the same IDF score, the features should be sorted by alphabetical order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d22a3960382cfe73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf_ngrams_sorted_by_idf(X_train, top_features=30):\n",
    "    \"\"\"Fits a TfidfVectorizer and returns its features sorted by \n",
    "    idf score\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Vectorized text data for training\n",
    "        top_features: maximum number of features to use \n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: used tf-idf vectorizer \n",
    "        ngrams_sorted (list): The ngrams of fitted_vectorizer sorted in descending order\n",
    "                              by their idf score. In case of tie, the features should be sorted \n",
    "                              by alphabetical order\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=top_features)\n",
    "    \n",
    "    vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    idf = vectorizer.idf_\n",
    "    \n",
    "    d = dict(zip(feature_names, idf))\n",
    "   \n",
    "    sorted_items = sorted(d.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    sorted_d = dict(sorted_items)\n",
    "\n",
    "    \n",
    "    ngrams_sorted = list(sorted_d.keys())\n",
    "    \n",
    "    return vectorizer, ngrams_sorted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8188311a1f32f69",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(df_preprocessed['reviewText'], top_features=100)\n",
    "assert hashlib.sha256(\"\".join(sorted_n_grams).encode()).hexdigest()=='7297438d39cfaa16f34e06b887d2e26b39234ed6a059ad9b954e32ad9420cc59', 'The features are not correct. Did you sort them correctly?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adca65f62fff79d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the features with top inverse document frequency in your vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd01e244d60c3db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 most secific features:\n",
      "['quot', 'war', 'young', 'loved', 'seems', 'stories', 'point', 'must', 'ever', 'old', 'pages', 'different', 'quite', 'come', 'family', 'without', 'last', 'feel', 'times', 'enjoyed', 'enough', 'bit', 'history', 'give', 'right', 'every', 'may', 'part', 'recommend', 'take']\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#show most specifc 30 features of the 100 selected features\n",
    "vectorizer, sorted_n_grams = get_tfidf_ngrams_sorted_by_idf(X_train, top_features=100)\n",
    "print('30 most secific features:')\n",
    "print(sorted_n_grams[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e6234e9cbfdcc8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we're only seeing the top IDF features, they tend to be more specific words you would find only in a subset of the documents. But in general, all of these features are not surprising in the context of books. \n",
    "\n",
    "### Q2.b)\n",
    "\n",
    "Finally we want to see how this behaves in comparison with our baseline. Use the functions you have defined above to try out different values of features (50, 100, 500, 1000, 2000, 5000 and 10000) in the training of a Multinomial Naive Bayes. Check the precision and recall of each run and store it in a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-641b26a61917cf52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "# For the following values: 50, 100, 500, 1000, 2000, 5000 and 10000\n",
    "# 1. Fit a TF-IDF limiting its features \n",
    "# 2. Train a Naive Bayes model\n",
    "# 3. Obtain precision and recall\n",
    "# 4. Store the lists of feature length, precision and recall values \n",
    "feature_lengths = [50, 100, 500, 1000, 2000, 5000, 10000]\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "\n",
    "for feature in feature_lengths:\n",
    "    vectorizer = TfidfVectorizer(max_features=feature)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_vec)\n",
    "    precision = precision_score(y_test, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d190a2151e63bd0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(feature_lengths) == len(precision_values) == len(recall_values) == 7, 'Did you use all the proposed numbers of features?'\n",
    "\n",
    "assert math.isclose(precision_values[4], 0.6114101184068891, rel_tol=1e-05), 'Some precision values are not correct.'\n",
    "assert math.isclose(recall_values[5], 0.7171339563862928, rel_tol=1e-05), 'Some recall values are not correct.'\n",
    "\n",
    "assert math.isclose(np.sum(precision_values), 4.170958590158336, rel_tol=1e-05), 'Some precision values are not correct.'\n",
    "assert math.isclose(np.sum(recall_values), 5.242990654205607, rel_tol=1e-05), 'Some recall values are not correct.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fda55a7260224715",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Look at the precision and recall variation with the number of features and answer the questions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44d6983aaea90cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n",
      "Precision: 0.5735359856951274\n",
      "Recall: 0.7993769470404984\n",
      "==============================\n",
      "Number of features: 100\n",
      "Precision: 0.5740824648844586\n",
      "Recall: 0.7894080996884735\n",
      "==============================\n",
      "Number of features: 500\n",
      "Precision: 0.5925\n",
      "Recall: 0.7383177570093458\n",
      "==============================\n",
      "Number of features: 1000\n",
      "Precision: 0.6041666666666666\n",
      "Recall: 0.7227414330218068\n",
      "==============================\n",
      "Number of features: 2000\n",
      "Precision: 0.6114101184068891\n",
      "Recall: 0.7077881619937695\n",
      "==============================\n",
      "Number of features: 5000\n",
      "Precision: 0.6132125732551944\n",
      "Recall: 0.7171339563862928\n",
      "==============================\n",
      "Number of features: 10000\n",
      "Precision: 0.60205078125\n",
      "Recall: 0.7682242990654206\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for n_features, precision, recall in zip(feature_lengths, precision_values, recall_values):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision}\")    \n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1192b685dad4fab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.i)** Which number of features yielded the **highest precision**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb3a3de21e03e7f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_highest_precision = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0dd5a122b8b0df3a",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_highest_precision)).hexdigest() == \\\n",
    "    \"7ca5bd879f393d9dd05b14f38add9c0fc6b67928f7f2d261b2e47a32ee8219e3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c2022411c99d37b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.ii)** Which number of features yielded the **highest recall**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98f7382e6d605724",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_highest_recall = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f164bd4d459f2790",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_highest_recall)).hexdigest() == \\\n",
    "    \"cc2786e1f9910a9d811400edcddaf7075195f7a16b216dcbefba3bc7c4f2ae51\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-01ec636c599a5df8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.iii)** Which number of features yielded the **highest F1-score**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd0c030243f5bfc5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_features_highest_f1 = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-cb3beeb2e54d7fe1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(bytes(num_features_highest_f1)).hexdigest() == \\\n",
    "    \"95b532cc4381affdff0d956e12520a04129ed49d37e154228368fe5621f0b9a2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1043b46f217fa6c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Q2.b.iv)** Knowing that you won't be able to read that many reviews, but you want to make sure the ones you do read are helpful, which model would you choose - the one with the highest precision, recall, or f1-score? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcf4ab6fc48580e9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "choice_model = 'precision' #, 'recall', 'f1-score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f5fb9e31f8c9a160",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(choice_model.encode()).hexdigest() == \\\n",
    "    '68c2f9ee314749c05c96df0cad305b0972506d78bb9b23c942cf805b274236c6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28f6e998fa1a40a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2.c)\n",
    "\n",
    "You also heard about Chi squared, another method that selects the best features based on their relevance.\n",
    "\n",
    "<img src=\"media/chi-squared-not-sure.jpg\" width=\"400\" />\n",
    "\n",
    "You decide to implement a function to run chi-squared over your vectorizer and return the selected features.\n",
    "\n",
    "**Note**: In this case, don't limit CountVectorizer/TfidfVectorizer to any number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0c66b2edf797342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_chi2(X_train, y_train, top_features=100, vectorizer_type=\"count\"):\n",
    "    \"\"\"Converts the provided text training data into feature counts. \n",
    "    Additionally, selects the best features with the chi squared method and returns\n",
    "    the sorted ngrams\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: training data\n",
    "        y_train: training labels\n",
    "        top_features: number of best features to select with the chi squared test\n",
    "        vectorizer_type: type of vectorizer to use (\"count\" or \"tfidf\")\n",
    "\n",
    "    Returns:\n",
    "        vectorizer: fitted vectorizer \n",
    "        ch2: fitted feature selector\n",
    "        X_train_ch2: transformed vector after feature selection\n",
    "        ngrams_sorted (list): the top features of the fitted vectorizer sorted in descending order\n",
    "                              by their chi squared score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the vectorizer\n",
    "    if vectorizer_type == \"count\":\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif vectorizer_type == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorizer_type. Must be 'count' or 'tfidf'.\")\n",
    "        \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Initialize the chi squared feature selector\n",
    "    ch2 = SelectKBest(chi2, k=top_features)\n",
    "    \n",
    "    # Fit the feature selector on the vectorized training data and target labels\n",
    "    X_train_ch2 = ch2.fit_transform(X_train_vec, y_train)\n",
    "       \n",
    "    # Get the feature names of the selected features\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    feature_scores = ch2.scores_\n",
    "    \n",
    "    # Create a dataframe of feature names and their corresponding chi squared scores\n",
    "    features = pd.DataFrame({\"feature\": feature_names, \"score\": feature_scores})\n",
    "    \n",
    "    # Sort the features by their chi squared scores\n",
    "    features.sort_values(by=[\"score\", \"feature\"], ascending=[False, True], inplace=True)\n",
    "    \n",
    "    # Get the top features\n",
    "    ngrams_sorted = features[\"feature\"].tolist()[:top_features]\n",
    "    \n",
    "    return vectorizer, ch2, X_train_ch2, ngrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'story',\n",
       " 'one',\n",
       " 'life',\n",
       " 'quot',\n",
       " 'also',\n",
       " 'world',\n",
       " 'well',\n",
       " 'first',\n",
       " 'people',\n",
       " 'novel',\n",
       " 'time',\n",
       " 'years',\n",
       " 'even',\n",
       " 'reader',\n",
       " 'many',\n",
       " 'two',\n",
       " 'mother',\n",
       " 'make',\n",
       " 'young',\n",
       " 'new',\n",
       " 'like',\n",
       " 'much',\n",
       " 'work',\n",
       " 'find',\n",
       " 'stories',\n",
       " 'may',\n",
       " 'home',\n",
       " 'family',\n",
       " 'us',\n",
       " 'way',\n",
       " 'man',\n",
       " 'living',\n",
       " 'several',\n",
       " 'town',\n",
       " 'old',\n",
       " 'author',\n",
       " 'set',\n",
       " 'characters',\n",
       " 'house',\n",
       " 'together',\n",
       " 'part',\n",
       " 'takes',\n",
       " 'little',\n",
       " 'war',\n",
       " 'help',\n",
       " 'read',\n",
       " 'history',\n",
       " 'sometimes',\n",
       " 'wright',\n",
       " 'series',\n",
       " 'wife',\n",
       " 'children',\n",
       " 'day',\n",
       " 'still',\n",
       " 'seem',\n",
       " 'english',\n",
       " 'end',\n",
       " 'makes',\n",
       " 'quite',\n",
       " 'ben',\n",
       " 'century',\n",
       " 'would',\n",
       " 'lot',\n",
       " 'found',\n",
       " 'see',\n",
       " 'others',\n",
       " 'best',\n",
       " 'text',\n",
       " 'want',\n",
       " 'human',\n",
       " 'woman',\n",
       " 'good',\n",
       " 'place',\n",
       " 'readers',\n",
       " 'become',\n",
       " 'events',\n",
       " 'personal',\n",
       " 'times',\n",
       " 'year',\n",
       " 'given',\n",
       " 'marriage',\n",
       " 'great',\n",
       " 'page',\n",
       " 'character',\n",
       " 'issues',\n",
       " 'john',\n",
       " 'get',\n",
       " 'although',\n",
       " 'long',\n",
       " 'friend',\n",
       " 'lives',\n",
       " 'section',\n",
       " 'father',\n",
       " 'white',\n",
       " 'death',\n",
       " 'live',\n",
       " 'friends',\n",
       " 'love',\n",
       " 'change',\n",
       " 'food',\n",
       " 'could',\n",
       " 'though',\n",
       " 'bit',\n",
       " 'marie',\n",
       " 'husband',\n",
       " 'take',\n",
       " 'memory',\n",
       " 'course',\n",
       " 'becomes',\n",
       " 'different',\n",
       " 'design',\n",
       " 'beautiful',\n",
       " 'back',\n",
       " 'highly',\n",
       " 'person',\n",
       " 'important',\n",
       " 'reading',\n",
       " 'fiction',\n",
       " 'rich',\n",
       " 'every',\n",
       " 'know',\n",
       " 'works']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features  = extract_features_chi2(X_train, y_train, vectorizer_type=\"count\", top_features=123)\n",
    "most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0b7f52fc1fdef287",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"tfidf\")\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[12, :]), 0.30615842862660636, rel_tol=1e-05), 'The transformed train data is not correct.'\n",
    "assert ch2.k == 100\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"tfidf\", top_features=123)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[:, 122]), 33.74087286012022, rel_tol=1e-05), 'The transformed train data is not correct.'\n",
    "assert ch2.k == 123\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features = extract_features_chi2(X_train, y_train, vectorizer_type=\"count\")\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[12, :]), 26), 'The transformed train data is not correct.'\n",
    "assert ch2.k == 100\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features  = extract_features_chi2(X_train, y_train, vectorizer_type=\"count\", top_features=123)\n",
    "\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[:, 122]), 734), 'The transformed train data is not correct.'\n",
    "assert ch2.k == 123\n",
    "\n",
    "assert len(most_important_features) == 123, 'The amount of features returned is wrong'\n",
    "assert hashlib.sha256(\"\".join(most_important_features).encode()).hexdigest()=='e2916147e2331b8b69e9bcab86285ed3b8eafe1ddd6def92fd39a19a528e9f13', 'The most important feature list is not correct. Have you sorted them correctly?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b6c9135c1144ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's apply our feature extractor with 30 features and check them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de4f48b0315a5c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book',\n",
       " 'story',\n",
       " 'one',\n",
       " 'life',\n",
       " 'quot',\n",
       " 'also',\n",
       " 'world',\n",
       " 'well',\n",
       " 'first',\n",
       " 'people',\n",
       " 'novel',\n",
       " 'time',\n",
       " 'years',\n",
       " 'even',\n",
       " 'reader',\n",
       " 'many',\n",
       " 'two',\n",
       " 'mother',\n",
       " 'make',\n",
       " 'young',\n",
       " 'new',\n",
       " 'like',\n",
       " 'much',\n",
       " 'work',\n",
       " 'find',\n",
       " 'stories',\n",
       " 'may',\n",
       " 'home',\n",
       " 'family',\n",
       " 'us']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, most_important_features  = extract_features_chi2(X_train, y_train, top_features=30)\n",
    "\n",
    "most_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-099ecaf666f3e62e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also see how often each of the selected features appears in the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75fa0016fc29f7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contains the word(s) \"book\"\n",
      "----\n",
      "1    3063\n",
      "0    2507\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"story\"\n",
      "----\n",
      "1    1705\n",
      "0    1164\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"one\"\n",
      "----\n",
      "1    2537\n",
      "0    1710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"life\"\n",
      "----\n",
      "1    1045\n",
      "0     565\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"quot\"\n",
      "----\n",
      "1    272\n",
      "0    134\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"also\"\n",
      "----\n",
      "1    937\n",
      "0    522\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"world\"\n",
      "----\n",
      "1    745\n",
      "0    375\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"well\"\n",
      "----\n",
      "1    1224\n",
      "0     710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"first\"\n",
      "----\n",
      "1    1079\n",
      "0     634\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"people\"\n",
      "----\n",
      "1    839\n",
      "0    487\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"novel\"\n",
      "----\n",
      "1    835\n",
      "0    484\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"time\"\n",
      "----\n",
      "1    1554\n",
      "0     943\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"years\"\n",
      "----\n",
      "1    633\n",
      "0    310\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"even\"\n",
      "----\n",
      "1    1289\n",
      "0     735\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"reader\"\n",
      "----\n",
      "1    785\n",
      "0    404\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"many\"\n",
      "----\n",
      "1    952\n",
      "0    544\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"two\"\n",
      "----\n",
      "1    730\n",
      "0    399\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"mother\"\n",
      "----\n",
      "1    316\n",
      "0    146\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"make\"\n",
      "----\n",
      "1    1087\n",
      "0     591\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"young\"\n",
      "----\n",
      "1    448\n",
      "0    198\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"new\"\n",
      "----\n",
      "1    919\n",
      "0    514\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"like\"\n",
      "----\n",
      "1    1668\n",
      "0    1158\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"much\"\n",
      "----\n",
      "1    1080\n",
      "0     710\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"work\"\n",
      "----\n",
      "1    985\n",
      "0    519\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"find\"\n",
      "----\n",
      "1    824\n",
      "0    450\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"stories\"\n",
      "----\n",
      "1    411\n",
      "0    224\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"may\"\n",
      "----\n",
      "1    667\n",
      "0    378\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"home\"\n",
      "----\n",
      "1    370\n",
      "0    152\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"family\"\n",
      "----\n",
      "1    435\n",
      "0    214\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n",
      "Documents that contains the word(s) \"us\"\n",
      "----\n",
      "1    2793\n",
      "0    1836\n",
      "Name: helpfulness, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word(s) \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train.str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1563c9a666e59d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems that most features selected as relevant are slightly more attached to the \"helpful\" class (1).\n",
    "\n",
    "Now let's see how the feature selection with chi squared translates to model training. Repeat the procedure you've done in Q2b to check the precision and recall of the models with chi squared selected features. Do that with the function you just created using the count vectorizer and chi-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38b468714a5ca798",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# For the following values: 50, 100, 500, 1000, 2000, 5000 and 10000\n",
    "# 1. Use the count vectorizer and ch2 feature selection with the given number of features \n",
    "# 2. Train a Naive Bayes model\n",
    "# 3. Obtain precision and recall\n",
    "# 4. Store the lists of feature length, precision and recall values \n",
    "\n",
    "feature_lengths = [50, 100, 500, 1000, 2000, 5000, 10000]\n",
    "precision_values = []\n",
    "recall_values = []\n",
    "\n",
    "# Iterate over the different feature values\n",
    "for feature in feature_lengths:\n",
    "    # Use the count vectorizer and ch2 feature selection with the given number of features\n",
    "    vectorizer, ch2, X_train_ch2, ngrams_sorted = extract_features_chi2(X_train, y_train, top_features=feature)\n",
    "    X_test_ch2 = ch2.transform(vectorizer.transform(X_test))\n",
    "    \n",
    "    # Train a Naive Bayes model\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_ch2, y_train)\n",
    "    \n",
    "    # Obtain precision and recall\n",
    "    y_pred = clf.predict(X_test_ch2)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the lists of feature length, precision and recall values\n",
    "    precision_values.append(precision)\n",
    "    recall_values.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0ad99d25ab050a2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(feature_lengths) == len(precision_values) == len(recall_values) == 7, 'Did you use all the proposed feature counts?'\n",
    "\n",
    "assert math.isclose(precision_values[4], 0.649458784346378, rel_tol=1e-05), 'Some precision values are not correct.'\n",
    "assert math.isclose(recall_values[5], 0.49221183800623053, rel_tol=1e-05), 'Some recall values are not correct.'\n",
    "\n",
    "assert math.isclose(np.sum(precision_values), 4.416826519514508, rel_tol=1e-05), 'Some precision values are not correct.'\n",
    "assert math.isclose(np.sum(recall_values), 3.5514018691588785, rel_tol=1e-05), 'Some recall values are not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-760cb8df07ac60fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, check the precision and recall scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n",
      "Precision: 0.5874079035498996\n",
      "Recall: 0.5464174454828661\n",
      "==============================\n",
      "Number of features: 100\n",
      "Precision: 0.6043795620437956\n",
      "Recall: 0.5158878504672897\n",
      "==============================\n",
      "Number of features: 500\n",
      "Precision: 0.630016051364366\n",
      "Recall: 0.48909657320872274\n",
      "==============================\n",
      "Number of features: 1000\n",
      "Precision: 0.6371753246753247\n",
      "Recall: 0.48909657320872274\n",
      "==============================\n",
      "Number of features: 2000\n",
      "Precision: 0.649458784346378\n",
      "Recall: 0.48598130841121495\n",
      "==============================\n",
      "Number of features: 5000\n",
      "Precision: 0.661641541038526\n",
      "Recall: 0.49221183800623053\n",
      "==============================\n",
      "Number of features: 10000\n",
      "Precision: 0.6467473524962178\n",
      "Recall: 0.5327102803738317\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for n_features, precision, recall in zip(feature_lengths, precision_values, recall_values):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision}\")    \n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1334fe8f60c2d3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this case, we were able to raise precision (check the values for 5000 features) even though we sacrificed recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8be788c4e7462464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "Now let's move on to more complex feature selection methods. In the previous methods, the selection is based on the actual n-grams, so we are limited to the information each feature brings in separately. We will now try PCA, which will find the principal components of our vectorized representation.\n",
    "\n",
    "Write a function that computes PCA on top of a CountVectorizer. Calculate the total variance explained by the PCA components (hint: use an attribute of PCA). Additionally, train a Support Vector Classifier on top of the PCA output.\n",
    "\n",
    "**To avoid using too much memory, and as we'll use dense matrices, please use a max_features value of 5000 in the vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Returns a fitted CountVectorizer, the PCA, a support vector classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        num_features (int): number of features to use in the PCA\n",
    "        seed (int): Seed to use for random state in PCA\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (CountVectorizer): CountVectorizer, fitted to X_train\n",
    "        pca (PCA): fitted PCA with the given number of components\n",
    "        clf (SVC): SVC classifier fitted to the PCA transformed training data\n",
    "        y_pred (Series): The predictions computed with the SVC classifier\n",
    "        explained_variance(float): variance in the data explained by all the PCA components together\n",
    "    \"\"\"\n",
    "    # Initialize the count vectorizer\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    pca = PCA(n_components=num_features, random_state=42)\n",
    "    \n",
    "    dense_X_train = X_train_vec.toarray()\n",
    "    dense_X_test = X_test_vec.toarray()\n",
    "    data_var = np.var(dense_X_train, axis=0).sum()\n",
    "\n",
    "    \n",
    "    pca.fit(dense_X_train)\n",
    "    \n",
    "    X_train_pca = pca.transform(dense_X_train)\n",
    "    X_test_pca = pca.transform(dense_X_test)\n",
    "    explained_variance = (1.0*np.var(X_train_pca, axis=0).sum() / data_var)\n",
    "    \n",
    "    \n",
    "    # Initialize the SVC classifier\n",
    "    clf = SVC(random_state=42)\n",
    "    \n",
    "    # Fit the classifier on the PCA transformed training data\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    \n",
    "    return vectorizer, pca, clf, y_pred, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e0c931cb07614c23",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Computed PCA for 30 features -----\n",
      "\n",
      "----- Computed PCA for 50 features -----\n",
      "\n",
      "----- Computed PCA for 100 features -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_preprocessed = load_preprocessed_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_preprocessed['reviewText'], \n",
    "    df_preprocessed['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "vectorizer, pca, clf, y_pred_30, variance_30 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=30, seed=42)\n",
    "print(\"----- Computed PCA for 30 features -----\\n\")\n",
    "assert np.mean(y_pred_30) == 0.5516666666666666, 'The prediction is not correct.'\n",
    "assert np.sum(y_pred_30) == 1655, 'The prediction is not correct.'\n",
    "assert pca.n_components == 30, 'The number of PCA components should be 30'\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching', 'The features are not correctly vectorized.'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_50, variance_50 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=50, seed=42)\n",
    "print(\"----- Computed PCA for 50 features -----\\n\")\n",
    "assert np.mean(y_pred_50) == 0.5416666666666666, 'The prediction is not correct.'\n",
    "assert np.sum(y_pred_50) == 1625, 'The prediction is not correct.'\n",
    "assert pca.n_components == 50, 'The number of PCA components should be 30'\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching', 'The features are not correctly vectorized.'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_100, variance_100 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42)\n",
    "print(\"----- Computed PCA for 100 features -----\\n\")\n",
    "assert np.mean(y_pred_100) == 0.544, 'The prediction is not correct.'\n",
    "assert np.sum(y_pred_100) == 1632, 'The prediction is not correct.'\n",
    "assert pca.n_components == 100, 'The number of PCA components should be 30'\n",
    "assert vectorizer.get_feature_names()[4424] == 'teaching','The features are not correctly vectorized.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1ecb5e0f82ec689",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can compare the new precision and recall with the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4362d8b3356bd257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions with 30 features: \n",
      "Precision: 0.656797583081571\n",
      "Recall: 0.6772585669781932\n",
      "Explained variance: 0.25141775667013727\n",
      "\n",
      "Predictions with 50 features: \n",
      "Precision: 0.6646153846153846\n",
      "Recall: 0.6728971962616822\n",
      "Explained variance: 0.3068693235464023\n",
      "\n",
      "Predictions with 100 features: \n",
      "Precision: 0.6611519607843137\n",
      "Recall: 0.6722741433021807\n",
      "Explained variance: 0.400755911573683\n"
     ]
    }
   ],
   "source": [
    "precision, recall = get_precision_recall(y_test, y_pred_30)     \n",
    "print(\"\\nPredictions with 30 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"Explained variance: {}\".format(variance_30))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_50)      \n",
    "print(\"\\nPredictions with 50 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"Explained variance: {}\".format(variance_50))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_100)     \n",
    "print(\"\\nPredictions with 100 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "print(\"Explained variance: {}\".format(variance_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98437874ea1e4117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nice! We kept the high precision of the chi squared feature selection method but with a much higher recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4.\n",
    "\n",
    "Now we'll change gears a bit and look into word vectors. In Learning Notebook 3 we told you that word vectors can be visualized after being projected into 2D space, and we showed you this diagram:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n",
    "\n",
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3847eb100057a7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b982ddaa87eec51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.a)\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"book\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "book_vector = nlp(\"book\").vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert book_vector.shape[0] == 300, 'The size of the vector is not correct.'\n",
    "assert math.isclose(book_vector.sum(), -8.673375, abs_tol=0.0001) #'This is not the correct vector.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7566702b1fd9755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.b) \n",
    "\n",
    "Next, write a function that uses sklearn's PCA to reduce the word vectors to a convenient number of dimensions for plotting. The function should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors, random_state):\n",
    "    \"\"\"\n",
    "    Returns PCA-reduced word vectors of the input vectors for plotting\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "        random_state (int): random state to use in PCA\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=2, random_state=random_state)\n",
    "    pca.fit(vectors)\n",
    "    reduced_vecs = pca.transform(vectors)\n",
    "    \n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors, random_state=42)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2), 'Did you choose the correct number of dimensions for the vectors?'\n",
    "assert math.isclose(reduced_vecs[1][1], 0.24736592153367926, rel_tol=1e-05), 'Some vectors are not reduced correctly.' \n",
    "assert math.isclose(reduced_vecs[2][0], 0.43388622222454437, rel_tol=1e-05), 'Some vectors are not reduced correctly.' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f75d7362f3b5b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll create an array of ~75,000 of spacy's word vectors and use your function to reduce them. If you're curious about using the full amount of word vectors, you can change the code to iterate over all vocab words - `list(nlp.vocab.strings)` - instead of our own `vocab_strings`, but beware that it will use a lot of memory!\n",
    "\n",
    "We'll also create a list of words that we'll plot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d7c693a88480b94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#words to plot later with reduced word vectors \n",
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing',\n",
    "                 'reading', 'studying', 'nintendo', 'sony', 'xbox', 'sad', 'angry', 'bored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-644c6807023bc77f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#reduce a subset of spacy word vectors\n",
    "with open('datasets/word_subset.txt') as fwords:\n",
    "    vocab_strings = fwords.read().splitlines()\n",
    "\n",
    "full_vocab_vecs = []\n",
    "for tok in vocab_strings:\n",
    "    full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors shape pre-PCA: (103202, 300)\n",
      "Word vectors shape after PCA: (103202, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array, random_state=42)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Time to plot! For this, we'll create a subset of the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9415c85fbda3dbb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#create a list of word vectors for the words_to_plot list\n",
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c059bc4a449b27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x640 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "min_x=min([x for x in coords_array[:,0]])\n",
    "max_x=max([x for x in coords_array[:,0]])\n",
    "min_y=min([y for y in coords_array[:,1]])\n",
    "max_y=max([y for y in coords_array[:,1]])\n",
    "plt.xlim(min_x,max_x)\n",
    "plt.ylim(min_y,max_y)\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "plt.plot([0,0],[min_y,max_y],'--',lw=0.7,c='lightblue')\n",
    "plt.plot([min_x,max_x],[0,0],'--',lw=0.7,c='lightblue')\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA has worked! In the diagram, we can see similar types of words closer together. Note that the closeness is measured by the angle between the vectors (from the origin to the blue endpoints). But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q4.c) \n",
    "\n",
    "As a final exercise, we'll look at word similarities.\n",
    "\n",
    "Write a function that returns the closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them. All the words are from the words_to_plot list defined in Q4b and the vectors are the 2D vectors reduced by PCA.\n",
    "\n",
    "Hint: you can use the already imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_vocab, word_vectors):\n",
    "    '''Returns a list of the closest words to the input_word from the word_in_vocab list,\n",
    "       based on cosine similarity of the word vectors\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word \n",
    "                             (the input_word is also from the words_in_vocab list)\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_vectors (np.array): 2D word vectors associated with the strings in words_in_vocab\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest words to the\n",
    "                                   input_word, based on cosine similarity between the word vectors\n",
    "    '''\n",
    "    input_word_vector = word_vectors[words_in_vocab.index(input_word)]\n",
    "    similarity_scores = np.dot(word_vectors, input_word_vector) / (np.linalg.norm(word_vectors, axis=1) * np.linalg.norm(input_word_vector))\n",
    "    closest_words_indices = np.argsort(similarity_scores)[-2:][::-1]\n",
    "    closest_words_list = [words_in_vocab[i] for i in closest_words_indices if words_in_vocab[i] != input_word]\n",
    "    return closest_words_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['studying']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_word('playing', words_to_plot, coords_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(closest_word('nintendo', words_to_plot, coords_array)[0].encode()).hexdigest() == 'f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73', 'This is not the closest word to nintendo'\n",
    "assert hashlib.sha256(closest_word('playing', words_to_plot, coords_array)[0].encode()).hexdigest() ==  '435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec', 'This is not the closest word to playing'\n",
    "assert hashlib.sha256(closest_word('pineapple', words_to_plot, coords_array)[0].encode()).hexdigest() == '6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494', 'This is not the closest word to pineapple'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Closest words to nintendo:\n",
      "['xbox']\n",
      "\n",
      "Closest words to playing:\n",
      "['studying']\n",
      "\n",
      "Closest words to pineapple:\n",
      "['mango']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClosest words to nintendo:\")\n",
    "print(closest_word('nintendo', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to playing:\")\n",
    "print(closest_word('playing', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to pineapple:\")\n",
    "print(closest_word('pineapple', words_to_plot, coords_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fdffae6311cd8b15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That is all, see you in the next specialization. \n",
    "\n",
    "PS: feel free to share your book reviews and recommendations. :)\n",
    "\n",
    "<img src=\"media/dont-buy-more-books.jpg\" width=\"50%\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
